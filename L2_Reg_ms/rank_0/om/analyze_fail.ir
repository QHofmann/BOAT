# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
The types of arguments in Map must be consistent, but the types of arguments are inconsistent.
There are 5 inputs of `map`, corresponding type info:
In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~825, 38~65
                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,
                                      ^
.
The type of the second argument in Map is: List[Ref[Tensor[Float32]]].
The type of the third argument in Map is: Tuple[Ref[Tensor[Float32]]].
The type of the 4th argument in Map is: Tuple[Ref[Tensor[Float32]]].
The type of the 5th argument in Map is: Tuple[Ref[Tensor[Float32]]].

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\ccsrc\frontend\operator\composite\map.cc:234 mindspore::prim::Map::Make

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:836~837, 8~64
        if not self.use_offload:
# 1 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:837, 12~64
            gradients = self.gradients_centralization(gradients)
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 2 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:848, 15~98
        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)
               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 3 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:740~825, 8~65
        if self.use_offload:
# 4 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:749~825, 12~65
            if self.use_dist_optimizer:
# 5 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:788~825, 16~65
                if self.is_group_lr:
# 6 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:808~825, 20~65
                    if self.use_lazy:
# 7 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:814~825, 24~65
                        if self.use_amsgrad:
# 8 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~825, 28~65
                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,
                            ^
# 9 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:814~825, 24~65
                        if self.use_amsgrad:
# 10 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:808~825, 20~65
                    if self.use_lazy:
# 11 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:788~825, 16~65
                if self.is_group_lr:
# 12 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:749~825, 12~65
            if self.use_dist_optimizer:
# 13 In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~825, 38~65
                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,
                                      ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_optim_adam_Adam_construct_25
# Total subgraphs: 0

# Attrs:
skip_auto_parallel_compile: 1

# Total params: 8
# Params:
%para1_gradients: <null>
%para2_global_step: <Ref[Tensor[Int32]], (1), ref_key=global_step>  :  has_default
%para3_beta1_power: <Ref[Tensor[Float32]], (), ref_key=beta1_power>  :  has_default
%para4_beta2_power: <Ref[Tensor[Float32]], (), ref_key=beta2_power>  :  has_default
%para5_x: <Ref[Tensor[Float32]], (130107), ref_key=x>  :  has_default
%para6_moment1.x: <Ref[Tensor[Float32]], (130107), ref_key=moment1.x>  :  has_default
%para7_moment2.x: <Ref[Tensor[Float32]], (130107), ref_key=moment2.x>  :  has_default
%para8_learning_rate: <Ref[Tensor[Float32]], (), ref_key=learning_rate>  :  has_default

subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_25 : 00000153800C9280
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:829~848, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_25() {
  %0(CNode_43) = resolve(NameSpace[Entry: 'mindspore.nn.optim.adam.Adam.construct'], mindspore.nn.optim.adam.Adam.construct)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
  %1(CNode_44) = MakeTuple(%para1_gradients)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<Tuple[List[Ref[Tensor[Float32]]]], TupleShape(ListShape[(130107)])>)
      #scope: (Default)
  %2(CNode_45) = MakeTuple()
      #scope: (Default)
  %3(CNode_46) = MakeTuple()
      #scope: (Default)
  %4(CNode_47) = make_dict(%2, %3)
      : (<Tuple[], TupleShape()>, <Tuple[], TupleShape()>) -> (<Dictionary[[],[]], NoShape>)
      #scope: (Default)

#------------------------> 0
  %5(CNode_48) = DoUnpackCall(%0, %1, %4)
      : (<Func, NoShape>, <Tuple[List[Ref[Tensor[Float32]]]], TupleShape(ListShape[(130107)])>, <Dictionary[[],[]], NoShape>) -> (<null>)
      #scope: (Default)
  Return(%5)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:836~837, 8~64/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_25:CNode_43{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Entry: 'mindspore.nn.optim.adam.Adam.construct', [2]: ValueNode<Symbol> mindspore.nn.optim.adam.Adam.construct}
#   2: @mindspore_nn_optim_adam_Adam_construct_25:CNode_48{[0]: ValueNode<Primitive> DoUnpackCall, [1]: CNode_43, [2]: CNode_44, [3]: CNode_47}
#   3: @mindspore_nn_optim_adam_Adam_construct_25:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_48}


subgraph attr:
core: 1
subgraph instance: UnpackCall_26 : 00000153800CEB80

subgraph @UnpackCall_26(%para0_Parameter_27, %para0_Parameter_28, %para0_Parameter_29) {
  %0(CNode_50) = TupleGetItem(%para0_Parameter_28, I64(0))
      : (<Tuple[List[Ref[Tensor[Float32]]]], TupleShape(ListShape[(130107)])>, <Int64, NoShape>) -> (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>)
      #scope: (Default)

#------------------------> 1
  %1(CNode_51) = Parameter_27(%0)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default)
}
# Order:
#   1: @UnpackCall_26:CNode_51{[0]: param_Parameter_27, [1]: CNode_50}
#   2: @UnpackCall_26:CNode_52{[0]: ValueNode<Primitive> Return, [1]: CNode_51}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_25 : 00000153800C6B90
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:829~848, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_25(%para0_gradients) {

#------------------------> 2
  %0(CNode_53) = call @mindspore_nn_optim_adam_Adam_construct_30()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:836~837, 8~64/        if not self.use_offload:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:836~837, 8~64/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_25:params{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> _parameters}
#   2: @mindspore_nn_optim_adam_Adam_construct_25:moment1{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> moment1}
#   3: @mindspore_nn_optim_adam_Adam_construct_25:moment2{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> moment2}
#   4: @mindspore_nn_optim_adam_Adam_construct_25:CNode_54{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> flatten_gradients}
#   5: @mindspore_nn_optim_adam_Adam_construct_25:CNode_55{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   7: @mindspore_nn_optim_adam_Adam_construct_25:gradients{[0]: CNode_54, [1]: param_gradients}
#   8: @mindspore_nn_optim_adam_Adam_construct_25:CNode_56{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> decay_weight}
#   9: @mindspore_nn_optim_adam_Adam_construct_25:CNode_57{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  11: @mindspore_nn_optim_adam_Adam_construct_25:gradients{[0]: CNode_56, [1]: gradients}
#  12: @mindspore_nn_optim_adam_Adam_construct_25:CNode_53{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_30}
#  13: @mindspore_nn_optim_adam_Adam_construct_25:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_53}
#  14: @mindspore_nn_optim_adam_Adam_construct_25:CNode_58{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> gradients_centralization}
#  15: @mindspore_nn_optim_adam_Adam_construct_25:CNode_59{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> scale_grad}
#  16: @mindspore_nn_optim_adam_Adam_construct_25:CNode_60{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> _grad_sparse_indices_deduplicate}
#  17: @mindspore_nn_optim_adam_Adam_construct_25:CNode_61{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> get_lr}
#  18: @mindspore_nn_optim_adam_Adam_construct_25:CNode_62{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> global_step}
#  19: @mindspore_nn_optim_adam_Adam_construct_25:CNode_63{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> global_step_increase_tensor}
#  20: @mindspore_nn_optim_adam_Adam_construct_25:CNode_64{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> beta1_power}
#  21: @mindspore_nn_optim_adam_Adam_construct_25:CNode_65{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> beta1}
#  22: @mindspore_nn_optim_adam_Adam_construct_25:CNode_66{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> beta2_power}
#  23: @mindspore_nn_optim_adam_Adam_construct_25:CNode_67{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> beta2}
#  24: @mindspore_nn_optim_adam_Adam_construct_25:CNode_68{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> _apply_adam}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_30 : 00000153800CAE50
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:829~848, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_30 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_25]() {

#------------------------> 3
  %0(CNode_69) = call @mindspore_nn_optim_adam_Adam_construct_31()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:837, 12~64/            gradients = self.gradients_centralization(gradients)/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:837, 12~64/            gradients = self.gradients_centralization(gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_30:CNode_70{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   3: @mindspore_nn_optim_adam_Adam_construct_30:gradients{[0]: CNode_58, [1]: gradients}
#   4: @mindspore_nn_optim_adam_Adam_construct_30:CNode_69{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_31}
#   5: @mindspore_nn_optim_adam_Adam_construct_30:CNode_71{[0]: ValueNode<Primitive> Return, [1]: CNode_69}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_31 : 00000153800CB3E0
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:829~848, 4~98/    @jit/
subgraph @mindspore_nn_optim_adam_Adam_construct_31 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_30]() {
  %0(CNode_72) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], assignadd)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:841, 8~22/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %1(CNode_62) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], global_step)
      : (<External, NoShape>, <External, NoShape>) -> (<Ref[Tensor[Int32]], (1)>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:841, 23~39/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_63) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], global_step_increase_tensor)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Int32], (1)>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:841, 41~73/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %3(CNode_73) = %0(%1, %2)
      : (<Ref[Tensor[Int32]], (1)>, <Tensor[Int32], (1)>) -> (<Ref[Tensor[Int32]], (1)>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:841, 8~74/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %4(CNode_64) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], beta1_power)
      : (<External, NoShape>, <External, NoShape>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:843, 22~38/        beta1_power = self.beta1_power * self.beta1/
  %5(CNode_74) = resolve(NameSpace[Ast: 'Namespace:mindspore._extends.parse.trope'], mul)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:843, 22~51/        beta1_power = self.beta1_power * self.beta1/
  %6(CNode_65) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], beta1)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:843, 41~51/        beta1_power = self.beta1_power * self.beta1/
  %7(beta1_power) = %5(%4, %6)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:843, 22~51/        beta1_power = self.beta1_power * self.beta1/
  %8(CNode_75) = call @assign_76(%4, %7)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:844, 8~38/        self.beta1_power = beta1_power/
  %9(CNode_66) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], beta2_power)
      : (<External, NoShape>, <External, NoShape>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:845, 22~38/        beta2_power = self.beta2_power * self.beta2/
  %10(CNode_77) = resolve(NameSpace[Ast: 'Namespace:mindspore._extends.parse.trope'], mul)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:845, 22~51/        beta2_power = self.beta2_power * self.beta2/
  %11(CNode_67) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], beta2)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:845, 41~51/        beta2_power = self.beta2_power * self.beta2/
  %12(beta2_power) = %10(%9, %11)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:845, 22~51/        beta2_power = self.beta2_power * self.beta2/
  %13(CNode_78) = call @assign_76(%9, %12)
      : (<Ref[Tensor[Float32]], ()>, <Tensor[Float32], ()>) -> (<Ref[Tensor[Float32]], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:846, 8~38/        self.beta2_power = beta2_power/
  %14(CNode_79) = MakeTuple(%3, %8, %13)
      : (<Ref[Tensor[Int32]], (1)>, <Ref[Tensor[Float32]], ()>, <Ref[Tensor[Float32]], ()>) -> (<Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:829~848, 4~98/    @jit/
  %15(CNode_80) = StopGradient(%14)
      : (<Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>) -> (<Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:829~848, 4~98/    @jit/
  %16(CNode_68) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], _apply_adam)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:848, 15~31/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %17(params) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], _parameters)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:831, 17~33/        params = self._parameters/
  %18(moment1) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], moment1)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:832, 18~30/        moment1 = self.moment1/
  %19(moment2) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], moment2)
      : (<External, NoShape>, <External, NoShape>) -> (<Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:833, 18~30/        moment2 = self.moment2/
  %20(CNode_61) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], get_lr)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:840, 13~24/        lr = self.get_lr()/
  %21(lr) = %20()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:840, 13~26/        lr = self.get_lr()/
  %22(CNode_60) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], _grad_sparse_indices_deduplicate)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:839, 20~57/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  %23(CNode_59) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], scale_grad)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:838, 20~35/        gradients = self.scale_grad(gradients)/
  %24(CNode_58) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], gradients_centralization)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:837, 24~53/            gradients = self.gradients_centralization(gradients)/
  %25(CNode_56) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], decay_weight)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:835, 20~37/        gradients = self.decay_weight(gradients)/
  %26(CNode_54) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], flatten_gradients)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:834, 20~42/        gradients = self.flatten_gradients(gradients)/
  %27(gradients) = %26(%para0_gradients)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:834, 20~53/        gradients = self.flatten_gradients(gradients)/
  %28(gradients) = %25(%27)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:835, 20~48/        gradients = self.decay_weight(gradients)/
  %29(gradients) = %24(%28)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:837, 24~64/            gradients = self.gradients_centralization(gradients)/
  %30(gradients) = %23(%29)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:838, 20~46/        gradients = self.scale_grad(gradients)/
  %31(gradients) = %22(%30)
      : (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<List[Ref[Tensor[Float32]]], ListShape[(130107)]>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:839, 20~68/        gradients = self._grad_sparse_indices_deduplicate(gradients)/

#------------------------> 4
  %32(CNode_81) = %16(%17, %7, %12, %18, %19, %21, %31)
      : (<Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>, <Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>, <Ref[Tensor[Float32]], ()>, <List[Ref[Tensor[Float32]]], ListShape[(130107)]>) -> (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:848, 15~98/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %33(CNode_82) = Depend(%32, %15) primitive_attrs: {side_effect_propagate: I64(1)} cnode_attrs: {topo_sort_rhs_first: Bool(1)}
      : (<null>, <Tuple[Ref[Tensor[Int32]],Ref[Tensor[Float32]]*2], TupleShape((1), (), ())>) -> (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:848, 15~98/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  Return(%33)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:848, 8~98/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_31:CNode_83{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   3: @mindspore_nn_optim_adam_Adam_construct_31:gradients{[0]: CNode_59, [1]: gradients}
#   4: @mindspore_nn_optim_adam_Adam_construct_31:CNode_84{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   6: @mindspore_nn_optim_adam_Adam_construct_31:gradients{[0]: CNode_60, [1]: gradients}
#   7: @mindspore_nn_optim_adam_Adam_construct_31:lr{[0]: CNode_61}
#   8: @mindspore_nn_optim_adam_Adam_construct_31:CNode_72{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> assignadd}
#   9: @mindspore_nn_optim_adam_Adam_construct_31:CNode_85{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  11: @mindspore_nn_optim_adam_Adam_construct_31:CNode_73{[0]: CNode_72, [1]: CNode_62, [2]: CNode_63}
#  12: @mindspore_nn_optim_adam_Adam_construct_31:CNode_74{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Ast: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> mul}
#  13: @mindspore_nn_optim_adam_Adam_construct_31:beta1_power{[0]: CNode_74, [1]: CNode_64, [2]: CNode_65}
#  14: @mindspore_nn_optim_adam_Adam_construct_31:CNode_75{[0]: ValueNode<FuncGraph> assign_76, [1]: CNode_64, [2]: beta1_power}
#  15: @mindspore_nn_optim_adam_Adam_construct_31:CNode_77{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> Ast: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> mul}
#  16: @mindspore_nn_optim_adam_Adam_construct_31:beta2_power{[0]: CNode_77, [1]: CNode_66, [2]: CNode_67}
#  17: @mindspore_nn_optim_adam_Adam_construct_31:CNode_78{[0]: ValueNode<FuncGraph> assign_76, [1]: CNode_66, [2]: beta2_power}
#  18: @mindspore_nn_optim_adam_Adam_construct_31:CNode_86{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  20: @mindspore_nn_optim_adam_Adam_construct_31:CNode_81{[0]: CNode_68, [1]: params, [2]: beta1_power, [3]: beta2_power, [4]: moment1, [5]: moment2, [6]: lr, [7]: gradients}
#  22: @mindspore_nn_optim_adam_Adam_construct_31:CNode_87{[0]: ValueNode<Primitive> Return, [1]: CNode_82}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_32 : 00000153800CF6A0
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_32(%para0_params, %para0_beta1_power, %para0_beta2_power, %para0_moment1, %para0_moment2, %para0_lr, %para0_gradients) {

#------------------------> 5
  %0(CNode_88) = call @_apply_adam_33()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:740~825, 8~65/        if self.use_offload:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:740~825, 8~65/        if self.use_offload:/
}
# Order:
#   1: @_apply_adam_32:CNode_88{[0]: ValueNode<FuncGraph> _apply_adam_33}
#   2: @_apply_adam_32:CNode_89{[0]: ValueNode<Primitive> Return, [1]: CNode_88}
#   3: @_apply_adam_32:CNode_90{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> map_}
#   4: @_apply_adam_32:CNode_91{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.nn.optim.adam', [2]: ValueNode<Symbol> F}
#   5: @_apply_adam_32:CNode_92{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> SymbolStr: 'Namespace:mindspore.nn.optim.adam', [2]: ValueNode<Symbol> _adam_opt}
#   6: @_apply_adam_32:CNode_93{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> beta1}
#   7: @_apply_adam_32:CNode_94{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> beta2}
#   8: @_apply_adam_32:CNode_95{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> eps}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_33 : 00000153800C9DA0
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_33 parent: [subgraph @_apply_adam_32]() {

#------------------------> 6
  %0(CNode_96) = call @_apply_adam_34()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:749~825, 12~65/            if self.use_dist_optimizer:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:749~825, 12~65/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_33:CNode_96{[0]: ValueNode<FuncGraph> _apply_adam_34}
#   2: @_apply_adam_33:CNode_97{[0]: ValueNode<Primitive> Return, [1]: CNode_96}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_34 : 00000153800D1800
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_34 parent: [subgraph @_apply_adam_32]() {

#------------------------> 7
  %0(CNode_98) = call @_apply_adam_35()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:788~825, 16~65/                if self.is_group_lr:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:788~825, 16~65/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_34:CNode_98{[0]: ValueNode<FuncGraph> _apply_adam_35}
#   2: @_apply_adam_34:CNode_99{[0]: ValueNode<Primitive> Return, [1]: CNode_98}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_35 : 00000153800D0750
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_35 parent: [subgraph @_apply_adam_32]() {

#------------------------> 8
  %0(CNode_100) = call @_apply_adam_36()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:808~825, 20~65/                    if self.use_lazy:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:808~825, 20~65/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_35:CNode_100{[0]: ValueNode<FuncGraph> _apply_adam_36}
#   2: @_apply_adam_35:CNode_101{[0]: ValueNode<Primitive> Return, [1]: CNode_100}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_36 : 00000153800D5530
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_36 parent: [subgraph @_apply_adam_32]() {

#------------------------> 9
  %0(CNode_102) = call @_apply_adam_37()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:814~825, 24~65/                        if self.use_amsgrad:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:814~825, 24~65/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_36:CNode_102{[0]: ValueNode<FuncGraph> _apply_adam_37}
#   2: @_apply_adam_36:CNode_103{[0]: ValueNode<Primitive> Return, [1]: CNode_102}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_37 : 00000153800D2320
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_37 parent: [subgraph @_apply_adam_32]() {

#------------------------> 10
  %0(CNode_104) = call @_apply_adam_38()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~825, 28~65/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~825, 28~65/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
}
# Order:
#   1: @_apply_adam_37:CNode_105{[0]: ValueNode<Primitive> getattr, [1]: CNode_91, [2]: ValueNode<StringImm> partial}
#   2: @_apply_adam_37:CNode_106{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> opt}
#   3: @_apply_adam_37:CNode_107{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> sparse_opt}
#   4: @_apply_adam_37:CNode_108{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> use_locking}
#   5: @_apply_adam_37:CNode_109{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> use_nesterov}
#   6: @_apply_adam_37:CNode_110{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>', [2]: ValueNode<Symbol> _is_device}
#   7: @_apply_adam_37:CNode_111{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#   9: @_apply_adam_37:CNode_112{[0]: CNode_105, [1]: CNode_92, [2]: CNode_106, [3]: CNode_107, [4]: CNode_108, [5]: CNode_109, [6]: CNode_110, [7]: param_beta1_power, [8]: param_beta2_power, [9]: CNode_93, [10]: CNode_94, [11]: CNode_95, [12]: param_lr}
#  10: @_apply_adam_37:CNode_113{[0]: ValueNode<Primitive> resolve, [1]: ValueNode<NameSpace> CommonOPS: 'Namespace:mindspore._extends.parse.trope', [2]: ValueNode<Symbol> MakeTuple}
#  12: @_apply_adam_37:success{[0]: CNode_90, [1]: CNode_112, [2]: param_gradients, [3]: param_params, [4]: param_moment1, [5]: param_moment2}
#  13: @_apply_adam_37:CNode_104{[0]: ValueNode<FuncGraph> _apply_adam_38}
#  14: @_apply_adam_37:CNode_114{[0]: ValueNode<Primitive> Return, [1]: CNode_104}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_38 : 00000153800D1270
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_38 parent: [subgraph @_apply_adam_37]() {

#------------------------> 11
  %0(CNode_115) = call @_apply_adam_39()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:814~825, 24~65/                        if self.use_amsgrad:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:814~825, 24~65/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_38:CNode_115{[0]: ValueNode<FuncGraph> _apply_adam_39}
#   2: @_apply_adam_38:CNode_116{[0]: ValueNode<Primitive> Return, [1]: CNode_115}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_39 : 00000153800D0CE0
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_39 parent: [subgraph @_apply_adam_37]() {

#------------------------> 12
  %0(CNode_117) = call @_apply_adam_40()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:808~825, 20~65/                    if self.use_lazy:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:808~825, 20~65/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_39:CNode_117{[0]: ValueNode<FuncGraph> _apply_adam_40}
#   2: @_apply_adam_39:CNode_118{[0]: ValueNode<Primitive> Return, [1]: CNode_117}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_40 : 00000153800CA330
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_40 parent: [subgraph @_apply_adam_37]() {

#------------------------> 13
  %0(CNode_119) = call @_apply_adam_41()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:788~825, 16~65/                if self.is_group_lr:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:788~825, 16~65/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_40:CNode_119{[0]: ValueNode<FuncGraph> _apply_adam_41}
#   2: @_apply_adam_40:CNode_120{[0]: ValueNode<Primitive> Return, [1]: CNode_119}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_41 : 00000153800D01C0
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_41 parent: [subgraph @_apply_adam_37]() {

#------------------------> 14
  %0(CNode_121) = call @_apply_adam_42()
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:749~825, 12~65/            if self.use_dist_optimizer:/
  Return(%0)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:749~825, 12~65/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_41:CNode_121{[0]: ValueNode<FuncGraph> _apply_adam_42}
#   2: @_apply_adam_41:CNode_122{[0]: ValueNode<Primitive> Return, [1]: CNode_121}


subgraph attr:
skip_auto_parallel_compile: 1
subgraph instance: _apply_adam_42 : 00000153800CFC30
# In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:738~827, 4~22/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_42 parent: [subgraph @_apply_adam_37]() {
  %0(CNode_90) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], map_)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821, 38~47/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %1(CNode_91) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.nn.optim.adam'], F)
      : (<External, NoShape>, <External, NoShape>) -> (<External, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821, 48~49/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %2(CNode_105) = getattr(%1, "partial")
      : (<External, NoShape>, <String, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821, 48~57/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %3(CNode_92) = resolve(NameSpace[SymbolStr: 'Namespace:mindspore.nn.optim.adam'], _adam_opt)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821, 58~67/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %4(CNode_106) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], opt)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821, 69~77/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %5(CNode_107) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], sparse_opt)
      : (<External, NoShape>, <External, NoShape>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821, 79~94/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  %6(CNode_108) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], use_locking)
      : (<External, NoShape>, <External, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:822, 58~74/                                                          self.use_locking, self.use_nesterov,/
  %7(CNode_109) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], use_nesterov)
      : (<External, NoShape>, <External, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:822, 76~93/                                                          self.use_locking, self.use_nesterov,/
  %8(CNode_110) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], _is_device)
      : (<External, NoShape>, <External, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:823, 58~73/                                                          self._is_device, beta1_power, beta2_power,/
  %9(CNode_93) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], beta1)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:824, 58~68/                                                          self.beta1, self.beta2, self.eps, lr), gradients, params,/
  %10(CNode_94) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], beta2)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:824, 70~80/                                                          self.beta1, self.beta2, self.eps, lr), gradients, params,/
  %11(CNode_95) = resolve(NameSpace[ClassMember: 'Namespace:mindspore.nn.optim.adam..<Adam::1460190682224>'], eps)
      : (<External, NoShape>, <External, NoShape>) -> (<Tensor[Float32], ()>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:824, 82~90/                                                          self.beta1, self.beta2, self.eps, lr), gradients, params,/
  %12(CNode_112) = %2(%3, %4, %5, %6, %7, %8, $(@_apply_adam_32:para0_beta1_power), $(@_apply_adam_32:para0_beta2_power), %9, %10, %11, $(@_apply_adam_32:para0_lr))
      : (<Func, NoShape>, <Func, NoShape>, <Func, NoShape>, <Bool, NoShape>, <Bool, NoShape>, <Bool, NoShape>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Tensor[Float32], ()>, <Ref[Tensor[Float32]], ()>) -> (<Func, NoShape>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~824, 48~95/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/

#------------------------> 15
  %13(success) = %0(%12, $(@_apply_adam_32:para0_gradients), $(@_apply_adam_32:para0_params), $(@_apply_adam_32:para0_moment1), $(@_apply_adam_32:para0_moment2))
      : (<Func, NoShape>, <List[Ref[Tensor[Float32]]], ListShape[(130107)]>, <Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>, <Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>, <Tuple[Ref[Tensor[Float32]]], TupleShape((130107))>) -> (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:821~825, 38~65/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt,/
  Return(%13)
      : (<null>)
      #scope: (Default)
      # In file C:\ProgramData\anaconda3\envs\mindspore\lib\site-packages\mindspore\nn\optim\adam.py:827, 8~22/        return success/
}
# Order:
#   1: @_apply_adam_42:CNode_123{[0]: ValueNode<Primitive> Return, [1]: success}


# ===============================================================================================
# The total of function graphs in evaluation stack: 16/18 (Ignored 2 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
No more function graphs.

