

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>boat_jit.higher_jit.optim &mdash; BOAT 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=f6245a2f"></script>
      <script src="../../../_static/doctools.js?v=888ff710"></script>
      <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            BOAT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation Guide:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../description.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install_guide.html">Installation and Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../boat_jit.html">boat_jit package</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Example:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../l2_regularization_example.html">L2 Regularization with Jittor</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">BOAT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
          <li class="breadcrumb-item"><a href="../higher_jit.html">boat_jit.higher_jit</a></li>
      <li class="breadcrumb-item active">boat_jit.higher_jit.optim</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for boat_jit.higher_jit.optim</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Facebook, Inc. and its affiliates.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;Differentiable optimizer wrappers around ``jittor.optim`` instances.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_abc</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_collections</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_typing</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_warnings</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">jittor</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jit</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">patch</span> <span class="k">as</span> <span class="n">_patch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">_utils</span>

<span class="n">_GroupedGradsType</span> <span class="o">=</span> <span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]]</span>
<span class="n">_StateType</span> <span class="o">=</span> <span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">DefaultDict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]]</span>
<span class="n">_GradClosureType</span> <span class="o">=</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]</span>
<span class="n">_OverrideType</span> <span class="o">=</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]]</span>
<span class="n">_GradCallbackType</span> <span class="o">=</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Callable</span><span class="p">[[</span><span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]],</span> <span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_mask_closure</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_GradClosureType</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">closure</span><span class="p">(</span><span class="n">grad</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span> <span class="n">grad</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">_get_mask_closure</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">grad</span>

    <span class="k">return</span> <span class="n">closure</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_maybe_mask</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
        <span class="n">tensor</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">_get_mask_closure</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span>


<div class="viewcode-block" id="DifferentiableOptimizer"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableOptimizer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableOptimizer</span><span class="p">(</span><span class="n">_abc</span><span class="o">.</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">other</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">reference_params</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Iterable</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span>
        <span class="n">fmodel</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_patch</span><span class="o">.</span><span class="n">_MonkeyPatchBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">override</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_OverrideType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grad_callback</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_GradCallbackType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">track_higher_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Initialize the optimizer with the state of an existing optimizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            other: an existing optimizer instance.</span>
<span class="sd">            reference_params: an iterable over the parameters of the original</span>
<span class="sd">                model.</span>
<span class="sd">            fmodel (optional): a patched stateless module with a view on</span>
<span class="sd">                weights.</span>
<span class="sd">            device (optional): the device to cast state tensors to.</span>
<span class="sd">            override (optional): a dictionary mapping optimizer settings (i.e.</span>
<span class="sd">                those which would be passed to the optimizer constructor or</span>
<span class="sd">                provided within parameter groups) to either singleton lists of</span>
<span class="sd">                override values, or to a list of override values of length equal</span>
<span class="sd">                to the number of parameter groups. If a single override is</span>
<span class="sd">                provided for a keyword, it is used for all parameter groups. If</span>
<span class="sd">                a list is provided, the ``i``\ th element of the list overrides the</span>
<span class="sd">                corresponding setting in the ``i``\ th parameter group. This permits</span>
<span class="sd">                the passing of tensors requiring gradient to differentiable</span>
<span class="sd">                optimizers for use as optimizer settings.</span>
<span class="sd">            grad_callback: (optional) a single argument function which will be</span>
<span class="sd">                applied to a list of gradients of parameters, which respects the</span>
<span class="sd">                order specified by ``reference_params``. This can be used to</span>
<span class="sd">                apply a function, such as gradient clipping, to all (or a</span>
<span class="sd">                subset) of these gradients every time the step function is</span>
<span class="sd">                called. If this keyword argument is provided when calling the</span>
<span class="sd">                step method, its value will override the default specified here.</span>
<span class="sd">            track_higher_grads: if True, during unrolled optimization the graph</span>
<span class="sd">                be retained, and the fast weights will bear grad funcs, so as to</span>
<span class="sd">                permit backpropagation through the optimization process. Setting</span>
<span class="sd">                this to False allows the differentiable optimizer to be used in</span>
<span class="sd">                &quot;test mode&quot;, without potentially tracking higher order</span>
<span class="sd">                gradients. This can be useful when running the training loop at</span>
<span class="sd">                test time, e.g. in k-shot learning experiments, without</span>
<span class="sd">                incurring a significant memory overhead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reference_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">reference_params</span><span class="p">)</span>

        <span class="c1"># Copy param groups and set up structures for copy state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="n">_copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_group_to_param_list</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">_collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>
        <span class="p">]</span>

        <span class="c1"># Deal with override</span>
        <span class="k">if</span> <span class="n">override</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_apply_override</span><span class="p">(</span><span class="n">override</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_grad_callback</span> <span class="o">=</span> <span class="n">grad_callback</span>

        <span class="c1"># Initialize state</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">orig_group</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">local_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">param_idx</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">orig_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
                <span class="n">param_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>  <span class="c1"># Use the unique ID of the parameter</span>
                <span class="k">if</span> <span class="n">param_id</span> <span class="ow">in</span> <span class="n">other</span><span class="o">.</span><span class="n">_grad_map</span><span class="p">:</span>
                    <span class="c1"># Initialize state for the parameter</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">param_idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="n">_utils</span><span class="o">.</span><span class="n">_recursive_copy_and_cast</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">other</span><span class="o">.</span><span class="n">_grad_map</span><span class="p">[</span><span class="n">param_id</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">_find_param_in_list</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">reference_params</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Could not find parameter </span><span class="si">{</span><span class="n">param</span><span class="si">}</span><span class="s2"> in reference parameters.&quot;</span>
                    <span class="p">)</span>
                <span class="n">local_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_group_to_param_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">local_list</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fmodel</span> <span class="o">=</span> <span class="n">fmodel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_track_higher_grads</span> <span class="o">=</span> <span class="n">track_higher_grads</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_override</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">override</span><span class="p">:</span> <span class="n">_OverrideType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">override</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Sanity check</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Mismatch between the number of override tensors for &quot;</span>
                    <span class="s2">&quot;optimizer parameter </span><span class="si">{}</span><span class="s2"> and the number of &quot;</span>
                    <span class="s2">&quot;parameter groups.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">v</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span>

<div class="viewcode-block" id="DifferentiableOptimizer.step"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableOptimizer.step">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">loss</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Iterable</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">override</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_OverrideType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">grad_callback</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_GradCallbackType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Iterable</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Perform a model update.</span>

<span class="sd">        This would be used by replacing the normal sequence::</span>

<span class="sd">            opt.zero_grad()</span>
<span class="sd">            loss.backward()</span>
<span class="sd">            opt.step()</span>

<span class="sd">        with::</span>

<span class="sd">            diffopt.step(loss)</span>


<span class="sd">        Args:</span>
<span class="sd">            loss: the loss tensor.</span>
<span class="sd">            params (optional): the parameters with regard to which we measure</span>
<span class="sd">                the loss. These must be provided if the differentiable optimizer</span>
<span class="sd">                did not receive a patched model with a view over its own fast</span>
<span class="sd">                weights at initialisation. If there is such a model, and params</span>
<span class="sd">                are provided, they will overwrite the params of the encapsulated</span>
<span class="sd">                model.</span>
<span class="sd">            override (optional): a dictionary mapping optimizer settings (i.e.</span>
<span class="sd">                those which would be passed to the optimizer constructor or</span>
<span class="sd">                provided within parameter groups) to either singleton lists of</span>
<span class="sd">                override values, or to a list of override values of length equal</span>
<span class="sd">                to the number of parameter groups. If a single override is</span>
<span class="sd">                provided for a keyword, it is used for all parameter groups. If</span>
<span class="sd">                a list is provided, the ``i``\ th element of the list overrides</span>
<span class="sd">                the corresponding setting in the ``i``\ th parameter group. This</span>
<span class="sd">                permits the passing of tensors requiring gradient to</span>
<span class="sd">                differentiable optimizers for use as optimizer settings. Setting</span>
<span class="sd">                override here has highest precedence, i.e. it will override any</span>
<span class="sd">                tensors provided as override during the creation of the</span>
<span class="sd">                differentiable optimizer, where there is name clash.</span>
<span class="sd">            grad_callback: (optional) a single argument function which will be</span>
<span class="sd">                applied to a list of gradients of parameters, which respects the</span>
<span class="sd">                order specified by ``reference_params``. This can be used to</span>
<span class="sd">                apply a function, such as gradient clipping, to all (or a</span>
<span class="sd">                subset) of these gradients every time the step function is</span>
<span class="sd">                called. This callback overrides the default provided when</span>
<span class="sd">                constructing the differentiable optimizer.</span>


<span class="sd">        Returns:</span>
<span class="sd">            The updated parameters, which will individually have ``grad_fn``\ s</span>
<span class="sd">            of their own. If the optimizer has an encapsulated patched model,</span>
<span class="sd">            its view over its own fast weights will be updated with these</span>
<span class="sd">            params.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Deal with override</span>
        <span class="k">if</span> <span class="n">override</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_apply_override</span><span class="p">(</span><span class="n">override</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fmodel</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fmodel</span><span class="o">.</span><span class="n">fast_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;params kwarg must be passed to step if the differentiable &quot;</span>
                    <span class="s2">&quot;optimizer doesn&#39;t have a view on a patched model with &quot;</span>
                    <span class="s2">&quot;params.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fmodel</span><span class="o">.</span><span class="n">fast_params</span> <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">params</span>

        <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

        <span class="n">grad_targets</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">p</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">()</span> <span class="k">else</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span>
        <span class="p">]</span>

        <span class="n">all_grads</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
            <span class="n">loss</span><span class="p">,</span>
            <span class="n">grad_targets</span><span class="p">,</span>
            <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Jittor does not have allow_unused, retain_graph used here</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">grad_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">all_grads</span> <span class="o">=</span> <span class="n">grad_callback</span><span class="p">(</span><span class="n">all_grads</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">all_grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_callback</span><span class="p">(</span><span class="n">all_grads</span><span class="p">)</span>

        <span class="n">grouped_grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">group</span><span class="p">,</span> <span class="n">mapping</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_to_param_list</span><span class="p">):</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mapping</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">all_grads</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
            <span class="n">grouped_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_update</span><span class="p">(</span><span class="n">grouped_grads</span><span class="p">)</span>

        <span class="n">new_params</span> <span class="o">=</span> <span class="n">params</span><span class="p">[:]</span>
        <span class="k">for</span> <span class="n">group</span><span class="p">,</span> <span class="n">mapping</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_to_param_list</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">mapping</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_track_higher_grads</span><span class="p">:</span>
                    <span class="n">new_params</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_params</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fmodel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fmodel</span><span class="o">.</span><span class="n">update_params</span><span class="p">(</span><span class="n">new_params</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_params</span></div>

    <span class="nd">@_abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="DifferentiableSGD"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableSGD">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableSGD</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the SGD optimizer.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># 如果 group 中没有，默认为 0.0</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">dampening</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dampening&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">nesterov</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;nesterov&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="c1"># 遍历参数和梯度</span>
            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># 如果 weight_decay 不为 0，则对梯度进行正则化</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># 使用 self.state 管理动量相关状态</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="n">momentum</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># 初始化 momentum_buffer 如果不存在</span>
                    <span class="k">if</span> <span class="s2">&quot;momentum_buffer&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_state</span><span class="p">:</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                    <span class="n">buf</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">]</span>
                    <span class="n">buf</span> <span class="o">*=</span> <span class="n">momentum</span>
                    <span class="n">buf</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dampening</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">nesterov</span><span class="p">:</span>
                        <span class="c1"># 如果使用 Nesterov 动量</span>
                        <span class="n">g</span> <span class="o">+=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">buf</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">g</span> <span class="o">=</span> <span class="n">buf</span>

                    <span class="c1"># 更新状态</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span>

                <span class="c1"># 最终更新参数</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">g</span></div>


<div class="viewcode-block" id="DifferentiableAdam"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableAdam">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableAdam</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the Adam optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">amsgrad</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;amsgrad&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;betas&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">],</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>

                <span class="c1"># Update step count</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>

                <span class="c1"># Apply weight decay</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="n">exp_avg</span> <span class="o">*=</span> <span class="n">beta1</span>
                <span class="n">exp_avg</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>

                <span class="n">exp_avg_sq</span> <span class="o">*=</span> <span class="n">beta2</span>
                <span class="n">exp_avg_sq</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">g</span> <span class="o">*</span> <span class="n">g</span><span class="p">)</span>

                <span class="c1"># Bias correction terms</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">step</span>

                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="c1"># Maintain the max of all 2nd moment running avg till now</span>
                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="p">)</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_exp_avg_sq</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">jit</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span> <span class="o">+</span> <span class="n">eps</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">jit</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span> <span class="o">+</span> <span class="n">eps</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                <span class="c1"># Update parameters</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span>

                <span class="c1"># Save updated state</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableAdamW"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableAdamW">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableAdamW</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the AdamW optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">amsgrad</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;amsgrad&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;betas&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>  <span class="c1"># Typical default for AdamW</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">],</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span>

                <span class="c1"># Apply weight decay directly on the parameter (AdamW specific)</span>
                <span class="n">p</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">weight_decay</span>

                <span class="c1"># Update step count</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="n">exp_avg</span> <span class="o">*=</span> <span class="n">beta1</span>
                <span class="n">exp_avg</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>

                <span class="n">exp_avg_sq</span> <span class="o">*=</span> <span class="n">beta2</span>
                <span class="n">exp_avg_sq</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">g</span> <span class="o">*</span> <span class="n">g</span><span class="p">)</span>

                <span class="c1"># Bias correction terms</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="o">**</span><span class="n">step</span>

                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="c1"># Maintain the max of all 2nd moment running avg till now</span>
                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="p">)</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;max_exp_avg_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_exp_avg_sq</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">jit</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span> <span class="o">+</span> <span class="n">eps</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">jit</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span> <span class="o">+</span> <span class="n">eps</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                <span class="c1"># Update parameters</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="p">(</span><span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span>

                <span class="c1"># Save updated state</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableAdadelta"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableAdadelta">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableAdadelta</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the Adadelta optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;rho&quot;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;square_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;acc_delta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">square_avg</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;square_avg&quot;</span><span class="p">]</span>
                <span class="n">acc_delta</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;acc_delta&quot;</span><span class="p">]</span>

                <span class="c1"># Update step count</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Apply weight decay if needed</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># Update square_avg</span>
                <span class="n">square_avg</span> <span class="o">*=</span> <span class="n">rho</span>
                <span class="n">square_avg</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">g</span> <span class="o">*</span> <span class="n">g</span><span class="p">)</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;square_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">square_avg</span>

                <span class="c1"># Compute update step</span>
                <span class="n">std</span> <span class="o">=</span> <span class="p">(</span><span class="n">square_avg</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">acc_delta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">std</span> <span class="o">*</span> <span class="n">g</span>

                <span class="c1"># Update acc_delta</span>
                <span class="n">acc_delta</span> <span class="o">*=</span> <span class="n">rho</span>
                <span class="n">acc_delta</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">delta</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;acc_delta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc_delta</span>

                <span class="c1"># Update parameter</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">delta</span>

                <span class="c1"># Save updated state</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableAdagrad"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableAdagrad">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableAdagrad</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the Adagrad optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
            <span class="n">lr_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">sum_</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">]</span>

                <span class="c1"># Update step count</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>

                <span class="c1"># Apply weight decay if needed</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># Compute adjusted learning rate</span>
                <span class="n">clr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">lr_decay</span><span class="p">)</span>

                <span class="c1"># Update sum of squared gradients</span>
                <span class="n">sum_</span> <span class="o">+=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">g</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_</span>

                <span class="c1"># Compute parameter update</span>
                <span class="n">std</span> <span class="o">=</span> <span class="n">sum_</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">clr</span> <span class="o">*</span> <span class="n">g</span> <span class="o">/</span> <span class="n">std</span>

                <span class="c1"># Save updated state</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableAdamax"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableAdamax">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableAdamax</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the Adamax optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="mf">2e-3</span><span class="p">)</span>
            <span class="n">betas</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;betas&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_inf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span>
                <span class="n">exp_inf</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_inf&quot;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">betas</span>

                <span class="c1"># Update step count</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>

                <span class="c1"># Apply weight decay</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># Update biased first moment estimate</span>
                <span class="n">exp_avg</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">*</span> <span class="n">beta1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_avg</span>

                <span class="c1"># Update the exponentially weighted infinity norm</span>
                <span class="n">exp_inf</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">exp_inf</span> <span class="o">*</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">abs</span><span class="p">())</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;exp_inf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">exp_inf</span>

                <span class="c1"># Bias correction</span>
                <span class="n">bias_correction</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="o">**</span><span class="n">step</span>
                <span class="n">clr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction</span>

                <span class="c1"># Parameter update</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">clr</span> <span class="o">*</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="p">(</span><span class="n">exp_inf</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

                <span class="c1"># Save updated state</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableASGD"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableASGD">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableASGD</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the ASGD optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
            <span class="n">lambd</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
            <span class="n">t0</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;t0&quot;</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">)</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;eta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;ax&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">eta</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;eta&quot;</span><span class="p">]</span>
                <span class="n">mu</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">]</span>
                <span class="n">ax</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;ax&quot;</span><span class="p">]</span>

                <span class="c1"># Update step count</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">step</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>

                <span class="c1"># Apply weight decay</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># Decay term</span>
                <span class="n">p</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">lambd</span> <span class="o">*</span> <span class="n">eta</span>

                <span class="c1"># Update parameter</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">g</span>

                <span class="c1"># Averaging</span>
                <span class="k">if</span> <span class="n">mu</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">ax</span> <span class="o">+=</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">ax</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ax</span> <span class="o">=</span> <span class="n">p</span>

                <span class="c1"># Update eta and mu</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;eta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">lambd</span> <span class="o">*</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">step</span><span class="p">)</span> <span class="o">**</span> <span class="n">alpha</span><span class="p">)</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;mu&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>

                <span class="c1"># Save updated parameter and state</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;ax&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableRMSprop"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableRMSprop">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableRMSprop</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the RMSprop optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">_warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Differentiable RMSprop may suffer from gradient correctness issues. &quot;</span>
            <span class="s2">&quot;Consider verifying behavior for specific use cases.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">centered</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;centered&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;square_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">momentum</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">centered</span><span class="p">:</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;grad_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">square_avg</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;square_avg&quot;</span><span class="p">]</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Apply weight decay</span>
                <span class="k">if</span> <span class="n">weight_decay</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">g</span> <span class="o">+=</span> <span class="n">weight_decay</span> <span class="o">*</span> <span class="n">p</span>

                <span class="c1"># Update running average of squared gradients</span>
                <span class="n">square_avg</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">square_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span> <span class="o">*</span> <span class="n">g</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;square_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">square_avg</span>

                <span class="c1"># Prevent NaNs for zero values</span>
                <span class="k">if</span> <span class="n">jit</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">square_avg</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
                    <span class="n">square_avg</span> <span class="o">+=</span> <span class="n">eps</span>

                <span class="c1"># Calculate centered RMSprop if applicable</span>
                <span class="k">if</span> <span class="n">centered</span><span class="p">:</span>
                    <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;grad_avg&quot;</span><span class="p">]</span>
                    <span class="n">grad_avg</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_avg</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;grad_avg&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad_avg</span>
                    <span class="n">avg</span> <span class="o">=</span> <span class="p">(</span><span class="n">square_avg</span> <span class="o">-</span> <span class="n">grad_avg</span> <span class="o">*</span> <span class="n">grad_avg</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">avg</span> <span class="o">=</span> <span class="n">square_avg</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span>

                <span class="c1"># Momentum updates if enabled</span>
                <span class="k">if</span> <span class="n">momentum</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">buf</span> <span class="o">=</span> <span class="n">param_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="s2">&quot;momentum_buffer&quot;</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="n">buf</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">buf</span> <span class="o">+</span> <span class="n">g</span> <span class="o">/</span> <span class="n">avg</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">buf</span>
                    <span class="n">p</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">buf</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">g</span> <span class="o">/</span> <span class="n">avg</span>

                <span class="c1"># Save updated parameter and state</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<div class="viewcode-block" id="DifferentiableRprop"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.DifferentiableRprop">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DifferentiableRprop</span><span class="p">(</span><span class="n">DifferentiableOptimizer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A differentiable version of the Rprop optimizer for Jittor.</span>

<span class="sd">    This optimizer creates a gradient tape as it updates parameters.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">_warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Differentiable Rprop correctly yields zero second-order gradients, &quot;</span>
            <span class="s2">&quot;as only the sign of the gradient is used in updates. Future versions &quot;</span>
            <span class="s2">&quot;may include higher-order gradient approximations.&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">:</span> <span class="n">_GroupedGradsType</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">grouped_grads</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
            <span class="n">etaminus</span><span class="p">,</span> <span class="n">etaplus</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;etas&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
            <span class="n">step_size_min</span><span class="p">,</span> <span class="n">step_size_max</span> <span class="o">=</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;step_sizes&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">p_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">],</span> <span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">p</span><span class="o">.</span><span class="n">is_stop_grad</span><span class="p">():</span>
                    <span class="k">continue</span>

                <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Rprop does not support sparse gradients&quot;</span><span class="p">)</span>

                <span class="c1"># State initialization</span>
                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">p_idx</span><span class="p">,</span> <span class="p">{})</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">param_state</span><span class="p">:</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;prev&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span>
                        <span class="n">p</span><span class="p">,</span> <span class="n">group</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>

                <span class="n">prev_grad</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;prev&quot;</span><span class="p">]</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step_size&quot;</span><span class="p">]</span>

                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Determine gradient sign</span>
                <span class="n">sign</span> <span class="o">=</span> <span class="p">(</span><span class="n">g</span> <span class="o">*</span> <span class="n">prev_grad</span><span class="p">)</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span>
                <span class="n">sign_positive</span> <span class="o">=</span> <span class="n">sign</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="n">sign_negative</span> <span class="o">=</span> <span class="n">sign</span> <span class="o">&lt;</span> <span class="mi">0</span>

                <span class="c1"># Update step sizes</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">sign_positive</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">etaplus</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">sign_negative</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">etaminus</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">step_size</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">step_size_min</span><span class="p">,</span> <span class="n">step_size_max</span><span class="p">)</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;step_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_size</span>

                <span class="c1"># Zero out gradient where sign is negative</span>
                <span class="n">g</span> <span class="o">=</span> <span class="n">jit</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">sign_negative</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">g</span><span class="p">),</span> <span class="n">g</span><span class="p">)</span>

                <span class="c1"># Update parameters</span>
                <span class="n">p</span> <span class="o">-=</span> <span class="n">g</span><span class="o">.</span><span class="n">sign</span><span class="p">()</span> <span class="o">*</span> <span class="n">step_size</span>

                <span class="c1"># Save state</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;prev&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">group_idx</span><span class="p">][</span><span class="n">p_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_state</span></div>


<span class="n">_OptMappingType</span> <span class="o">=</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span>
    <span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Type</span><span class="p">[</span><span class="n">DifferentiableOptimizer</span><span class="p">]</span>
<span class="p">]</span>
<span class="c1"># _opt_mapping: _OptMappingType = {</span>
<span class="c1">#     jit.optim.Adadelta: DifferentiableAdadelta,</span>
<span class="c1">#     jit.optim.Adagrad: DifferentiableAdagrad,</span>
<span class="c1">#     jit.optim.Adam: DifferentiableAdam,</span>
<span class="c1">#     jit.optim.AdamW: DifferentiableAdamW,</span>
<span class="c1">#     jit.optim.Adamax: DifferentiableAdamax,</span>
<span class="c1">#     jit.optim.ASGD: DifferentiableASGD,</span>
<span class="c1">#     jit.optim.RMSprop: DifferentiableRMSprop,</span>
<span class="c1">#     jit.optim.Rprop: DifferentiableRprop,</span>
<span class="c1">#     jit.optim.SGD: DifferentiableSGD,</span>
<span class="c1"># }</span>

<span class="c1"># 获取实际存在的优化器</span>
<span class="n">available_optimizers</span> <span class="o">=</span> <span class="p">{</span><span class="n">attr</span> <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="nb">dir</span><span class="p">(</span><span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">attr</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)}</span>

<span class="c1"># 定义优化器映射</span>
<span class="n">_opt_mapping</span><span class="p">:</span> <span class="n">_OptMappingType</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Adadelta&quot;</span><span class="p">:</span> <span class="n">DifferentiableAdadelta</span><span class="p">,</span>
    <span class="s2">&quot;Adagrad&quot;</span><span class="p">:</span> <span class="n">DifferentiableAdagrad</span><span class="p">,</span>
    <span class="s2">&quot;Adam&quot;</span><span class="p">:</span> <span class="n">DifferentiableAdam</span><span class="p">,</span>
    <span class="s2">&quot;AdamW&quot;</span><span class="p">:</span> <span class="n">DifferentiableAdamW</span><span class="p">,</span>
    <span class="s2">&quot;Adamax&quot;</span><span class="p">:</span> <span class="n">DifferentiableAdamax</span><span class="p">,</span>
    <span class="s2">&quot;ASGD&quot;</span><span class="p">:</span> <span class="n">DifferentiableASGD</span><span class="p">,</span>
    <span class="s2">&quot;RMSprop&quot;</span><span class="p">:</span> <span class="n">DifferentiableRMSprop</span><span class="p">,</span>
    <span class="s2">&quot;Rprop&quot;</span><span class="p">:</span> <span class="n">DifferentiableRprop</span><span class="p">,</span>
    <span class="s2">&quot;SGD&quot;</span><span class="p">:</span> <span class="n">DifferentiableSGD</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># 移除不存在的优化器</span>
<span class="n">_opt_mapping</span> <span class="o">=</span> <span class="p">{</span>
    <span class="nb">getattr</span><span class="p">(</span><span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span> <span class="n">v</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">_opt_mapping</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">available_optimizers</span>
<span class="p">}</span>

<span class="c1"># print(&quot;Updated optimizer mapping:&quot;, _opt_mapping)</span>


<div class="viewcode-block" id="get_diff_optim"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.get_diff_optim">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_diff_optim</span><span class="p">(</span>
    <span class="n">opt</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">reference_params</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Iterable</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span>
    <span class="n">fmodel</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_patch</span><span class="o">.</span><span class="n">_MonkeyPatchBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">override</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_OverrideType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_higher_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DifferentiableOptimizer</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct/initialize a differentiable version of an existing optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        opt: an existing optimizer, assumed to be an instance of</span>
<span class="sd">            ``jittor.optim.Optimizer``, of a supported type which is either defined</span>
<span class="sd">            in ``jittor.optim``, or a custom implementation which has been added to</span>
<span class="sd">            higher at runtime by using ``higher.register_optim``. We assume this</span>
<span class="sd">            optimizer tracks the parameters (or some subset thereof) of a single</span>
<span class="sd">            ``jittor.Module`` instance, with support for parameter groups.</span>
<span class="sd">        reference_params: the parameters of the module tracked by ``opt``, as</span>
<span class="sd">            returned by ``module.parameters()``.</span>
<span class="sd">        fmodel (optional): a patched version of the ``module`` tracked by ``opt``.</span>
<span class="sd">            It is assumed this patched instance has a view on its latest fast</span>
<span class="sd">            weights through ``fmodel.parameters()``. If provided, it is not</span>
<span class="sd">            necessary to pass the fast weights explicitly to the differentiable</span>
<span class="sd">            optimizer&#39;s ``step`` function via the keyword arg ``params``. If not</span>
<span class="sd">            provided, the fast weights to update must be provided to ``step``.</span>
<span class="sd">        device (optional): the device to cast the optimizer state to when</span>
<span class="sd">            creating the differentiable optimizer. If not provided, the same</span>
<span class="sd">            device as used for the parameters tracked by ``opt`` will be used.</span>
<span class="sd">        override (optional): a dictionary mapping optimizer settings (i.e.</span>
<span class="sd">            those which would be passed to the optimizer constructor or</span>
<span class="sd">            provided within parameter groups) to either singleton lists of</span>
<span class="sd">            override values, or to a list of override values of length equal to</span>
<span class="sd">            the number of parameter groups. If a single override is provided for</span>
<span class="sd">            a keyword, it is used for all parameter groups. If a list is</span>
<span class="sd">            provided, the ``i``\ th element of the list overrides the corresponding</span>
<span class="sd">            setting in the ``i``\ th parameter group. This permits the passing of</span>
<span class="sd">            tensors requiring gradient to differentiable optimizers for use as</span>
<span class="sd">            optimizer settings.</span>
<span class="sd">        track_higher_grads: if True, during unrolled optimization the graph be</span>
<span class="sd">            retained, and the fast weights will bear grad funcs, so as to permit</span>
<span class="sd">            backpropagation through the optimization process. Setting this to</span>
<span class="sd">            False allows the returned differentiable optimizer to be used in</span>
<span class="sd">            &quot;test mode&quot;, without potentially tracking higher order gradients.</span>
<span class="sd">            This can be useful when running the training loop at test time,</span>
<span class="sd">            e.g. in k-shot learning experiments, without incurring a significant</span>
<span class="sd">            memory overhead.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An initialized ``DifferentiableOptimizer`` instance of the right subtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">opt</span><span class="p">)</span> <span class="ow">in</span> <span class="n">_opt_mapping</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_opt_mapping</span><span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="n">opt</span><span class="p">)](</span>
            <span class="n">opt</span><span class="p">,</span>
            <span class="n">reference_params</span><span class="p">,</span>
            <span class="n">fmodel</span><span class="o">=</span><span class="n">fmodel</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">override</span><span class="o">=</span><span class="n">override</span><span class="p">,</span>
            <span class="n">track_higher_grads</span><span class="o">=</span><span class="n">track_higher_grads</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Optimizer type </span><span class="si">{}</span><span class="s2"> not supported by higher yet.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">opt</span><span class="p">))</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="create_diff_optim"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.create_diff_optim">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">create_diff_optim</span><span class="p">(</span>
    <span class="n">opt_type</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Type</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span>
    <span class="n">opt_kwargs</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_typing</span><span class="o">.</span><span class="n">List</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fmodel</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_patch</span><span class="o">.</span><span class="n">_MonkeyPatchBase</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">override</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">_OverrideType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">track_higher_grads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DifferentiableOptimizer</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Construct a differentiable version of an new optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        opt_type: the type (constructor) for a jittor.optim.Optimizer subtype</span>
<span class="sd">            from amongst the types supported by the library, or registered with</span>
<span class="sd">            it a runtime.</span>
<span class="sd">        opt_kwargs: a dictionary of keywords to be passed to the optimizer</span>
<span class="sd">            constructor.</span>
<span class="sd">        params (optional): a list of (fast) weights which the differentiable</span>
<span class="sd">            optimizer will update. These must be provided if fmodel is not</span>
<span class="sd">            provided. If both, these will be used in lieu. These will only</span>
<span class="sd">            be used for shape inference when initializing the optimizer.</span>
<span class="sd">            This argument can also take the same format as parameter groups,</span>
<span class="sd">            i.e. an iterable over dictionaries which contain the &#39;params&#39; key</span>
<span class="sd">            with fast weights as value, and group-specific hyperparameters.</span>
<span class="sd">        fmodel (optional): a patched version of the ``module`` tracked by ``opt``.</span>
<span class="sd">            It is assumed this patched instance has a view on its latest fast</span>
<span class="sd">            weights through ``fmodel.parameters()``. If provided, it is not</span>
<span class="sd">            necessary to pass the fast weights explicitly to the differentiable</span>
<span class="sd">            optimizer&#39;s ``step`` function via the keyword arg ``params``. If not</span>
<span class="sd">            provided, the fast weights to update must be provided to ``step``.</span>
<span class="sd">        device (optional): the device to cast the optimizer state to when</span>
<span class="sd">            creating the differentiable optimizer. If not provided, the same</span>
<span class="sd">            device as used for the parameters tracked by ``opt`` will be used.</span>
<span class="sd">        override (optional): a dictionary mapping optimizer settings (i.e.</span>
<span class="sd">            those which would be passed to the optimizer constructor or</span>
<span class="sd">            provided within parameter groups) to either singleton lists of</span>
<span class="sd">            override values, or to a list of override values of length equal to</span>
<span class="sd">            the number of parameter groups. If a single override is provided for</span>
<span class="sd">            a keyword, it is used for all parameter groups. If a list is</span>
<span class="sd">            provided, the ``i``\ th element of the list overrides the corresponding</span>
<span class="sd">            setting in the ``i``\ th parameter group. This permits the passing of</span>
<span class="sd">            tensors requiring gradient to differentiable optimizers for use as</span>
<span class="sd">            optimizer settings.</span>
<span class="sd">        track_higher_grads: if True, during unrolled optimization the graph be</span>
<span class="sd">            retained, and the fast weights will bear grad funcs, so as to permit</span>
<span class="sd">            backpropagation through the optimization process. Setting this to</span>
<span class="sd">            False allows the returned differentiable optimizer to be used in</span>
<span class="sd">            &quot;test mode&quot;, without potentially tracking higher order gradients.</span>
<span class="sd">            This can be useful when running the training loop at test time,</span>
<span class="sd">            e.g. in k-shot learning experiments, without incurring a significant</span>
<span class="sd">            memory overhead.</span>

<span class="sd">    Returns:</span>
<span class="sd">        An initialized ``DifferentiableOptimizer`` instance of the right subtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">opt_type</span> <span class="ow">in</span> <span class="n">_opt_mapping</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">dummy</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="n">k</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;params&quot;</span> <span class="k">else</span> <span class="n">v</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">group</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">params</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dummy</span> <span class="o">=</span> <span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">fmodel</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dummy</span> <span class="o">=</span> <span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">fmodel</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must specify one of fmodel or params in kwargs.&quot;</span><span class="p">)</span>

        <span class="n">opt_kwargs</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">opt_kwargs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">opt_kwargs</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">opt_type</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="o">**</span><span class="n">opt_kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">_opt_mapping</span><span class="p">[</span><span class="n">opt_type</span><span class="p">](</span>
            <span class="n">opt</span><span class="p">,</span>
            <span class="n">dummy</span><span class="p">,</span>
            <span class="n">fmodel</span><span class="o">=</span><span class="n">fmodel</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">override</span><span class="o">=</span><span class="n">override</span><span class="p">,</span>
            <span class="n">track_higher_grads</span><span class="o">=</span><span class="n">track_higher_grads</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Optimizer type </span><span class="si">{}</span><span class="s2"> not supported by higher yet.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">opt_type</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="register_optim"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.register_optim">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">register_optim</span><span class="p">(</span>
    <span class="n">optim_type</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
    <span class="n">diff_optim_type</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Type</span><span class="p">[</span><span class="n">DifferentiableOptimizer</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a new optimizer type for use with higher functions.</span>

<span class="sd">    Args:</span>
<span class="sd">        optim_type: the type of a new optimizer, assumed to be an instance of</span>
<span class="sd">            ``jittor.optim.Optimizer``.</span>
<span class="sd">        diff_optim_type: the type of a new differentiable optimizer, assumed to</span>
<span class="sd">            be an instance of ``higher.optim.DifferentiableOptimizer`` with</span>
<span class="sd">            functionally equivalent logic to ``optim_type``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_opt_mapping</span><span class="p">[</span><span class="n">optim_type</span><span class="p">]</span> <span class="o">=</span> <span class="n">diff_optim_type</span></div>


<div class="viewcode-block" id="get_trainable_opt_params"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.get_trainable_opt_params">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_trainable_opt_params</span><span class="p">(</span>
    <span class="n">opt</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_OverrideType</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get an override dictionary from an optimizer instance.</span>

<span class="sd">    Args:</span>
<span class="sd">        opt: the optimizer to obtain an override dictionary from.</span>
<span class="sd">        device (optional): the device to cast the learnable tensors to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dictionary of the format expected for the override kwarg of</span>
<span class="sd">        differentiable optimizers. It is initialized with trainable tensors</span>
<span class="sd">        with as values those float and int hyperparameters found in the</span>
<span class="sd">        optimizer&#39;s parameter groups (or stuctures containing these).</span>
<span class="sd">        Heuristically, hyperparameters containing mixtures of differentiable</span>
<span class="sd">        and non-differentiable types will be ignored (and must be manually</span>
<span class="sd">        specified when constructing an override dict).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">override</span><span class="p">:</span> <span class="n">_OverrideType</span> <span class="o">=</span> <span class="n">_collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">map_fn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">jit</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">stop_grad</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">group</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;params&quot;</span><span class="p">:</span>
                <span class="c1"># Ignore actual model parameters tracked by optim</span>
                <span class="k">continue</span>

            <span class="c1"># Ignore hyperparameters that aren&#39;t structures containing ints</span>
            <span class="c1"># or floats</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">_utils</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">override</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">_utils</span><span class="o">.</span><span class="n">_recursive_map</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">map_fn</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">override</span></div>


<div class="viewcode-block" id="apply_trainable_opt_params"><a class="viewcode-back" href="../../../boat_jit.higher_jit.html#boat_jit.higher_jit.optim.apply_trainable_opt_params">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">apply_trainable_opt_params</span><span class="p">(</span>
    <span class="n">opt</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">override</span><span class="p">:</span> <span class="n">_OverrideType</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Apply learned hyperparameters back to original optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        opt: the original optimizer. The hyperparameters in its parameter groups</span>
<span class="sd">            will be modified in place.</span>
<span class="sd">        override: dictionary of the format used for the override kwarg of</span>
<span class="sd">            differentiable optimizers.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">override</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="c1"># Sanity check</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Mismatch between the number of override tensors for &quot;</span>
                <span class="s2">&quot;optimizer parameter </span><span class="si">{}</span><span class="s2"> and the number of &quot;</span>
                <span class="s2">&quot;parameter groups.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">group_idx</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">replacement</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">v</span><span class="p">[</span><span class="n">group_idx</span><span class="p">]</span>
            <span class="n">group</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">_recursive_apply</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="n">group</span><span class="p">[</span><span class="n">k</span><span class="p">])</span></div>


<span class="c1">## Local utility functions</span>
<span class="c1"># TODO(egrefen): use funcs below instead of x._add, in diffopt</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_add</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span>
    <span class="n">a1</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span>
    <span class="n">a2</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">a2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">a1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">a1</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">a2</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span> <span class="o">*</span> <span class="n">other</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_addcdiv</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span>
    <span class="n">a1</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span>
    <span class="n">a2</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span>
    <span class="n">a3</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">a3</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">tensor1</span> <span class="o">=</span> <span class="n">a1</span>
        <span class="n">tensor2</span> <span class="o">=</span> <span class="n">a2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">a1</span>
        <span class="n">tensor1</span> <span class="o">=</span> <span class="n">a2</span>
        <span class="n">tensor2</span> <span class="o">=</span> <span class="n">a3</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="o">+</span> <span class="n">value</span> <span class="o">*</span> <span class="p">(</span><span class="n">tensor1</span> <span class="o">/</span> <span class="n">tensor2</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_addcmul</span><span class="p">(</span>
    <span class="n">tensor</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span>
    <span class="n">a1</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span>
    <span class="n">a2</span><span class="p">:</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span>
    <span class="n">a3</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Optional</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">a3</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">tensor1</span> <span class="o">=</span> <span class="n">a1</span>
        <span class="n">tensor2</span> <span class="o">=</span> <span class="n">a2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">a1</span>
        <span class="n">tensor1</span> <span class="o">=</span> <span class="n">a2</span>
        <span class="n">tensor2</span> <span class="o">=</span> <span class="n">a3</span>
    <span class="k">return</span> <span class="n">tensor</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span> <span class="o">*</span> <span class="n">tensor1</span> <span class="o">*</span> <span class="n">tensor2</span><span class="p">)</span>


<span class="c1"># TODO(egrefen): this probably could be refactored into utils</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_recursive_apply</span><span class="p">(</span>
    <span class="n">replacement</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">],</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_typing</span><span class="o">.</span><span class="n">Union</span><span class="p">[</span><span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_utils</span><span class="o">.</span><span class="n">_is_container</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)(</span><span class="n">replacement</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Expected an non-container type for target, but got </span><span class="si">{}</span><span class="s2"> with value &quot;</span>
            <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">target</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">jit</span><span class="o">.</span><span class="n">Var</span><span class="p">):</span>
        <span class="n">replacement</span> <span class="o">=</span> <span class="n">replacement</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">replacement</span><span class="o">.</span><span class="n">data</span>
        <span class="k">return</span> <span class="n">target</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)(</span>
            <span class="p">[</span><span class="n">_recursive_apply</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="n">target</span><span class="p">)]</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)(</span>
            <span class="p">[</span><span class="n">_recursive_apply</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="n">target</span><span class="p">)]</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">_recursive_apply</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">r</span><span class="p">),</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">replacement</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="p">}</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">)(</span>
            <span class="p">{</span><span class="n">_recursive_apply</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">replacement</span><span class="p">,</span> <span class="n">target</span><span class="p">)}</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Couldn&#39;t apply replacement of type </span><span class="si">{}</span><span class="s2"> to target of type &quot;</span>
            <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">replacement</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
        <span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Yaohua Liu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>